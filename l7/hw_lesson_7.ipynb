{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_lesson_7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GbTxYia0H2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e8MdPjn3a1j",
        "colab_type": "text"
      },
      "source": [
        "## Обучаем seq2seq на уровне токенов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLxKaXTE3aip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 60\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = '/content/rus.txt'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pZTyfRi3agk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJVLI4wR3adG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAuyA3Vv3aaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emoOpr0Q3aQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de000e73-7812-4054-d22f-0af3a1c5684c"
      },
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "  1/125 [..............................] - ETA: 0s - loss: 4.5059 - accuracy: 0.0013WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0117s vs `on_train_batch_end` time: 0.0280s). Check your callbacks.\n",
            "125/125 [==============================] - 5s 40ms/step - loss: 1.1481 - accuracy: 0.7667 - val_loss: 0.9137 - val_accuracy: 0.7583\n",
            "Epoch 2/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.7379 - accuracy: 0.8021 - val_loss: 0.7827 - val_accuracy: 0.7943\n",
            "Epoch 3/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.6365 - accuracy: 0.8325 - val_loss: 0.6929 - val_accuracy: 0.8152\n",
            "Epoch 4/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.7008 - accuracy: 0.8247 - val_loss: 0.6965 - val_accuracy: 0.8103\n",
            "Epoch 5/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.5817 - accuracy: 0.8388 - val_loss: 0.6500 - val_accuracy: 0.8195\n",
            "Epoch 6/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.5484 - accuracy: 0.8458 - val_loss: 0.6246 - val_accuracy: 0.8244\n",
            "Epoch 7/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.5278 - accuracy: 0.8498 - val_loss: 0.6041 - val_accuracy: 0.8285\n",
            "Epoch 8/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.5105 - accuracy: 0.8540 - val_loss: 0.5885 - val_accuracy: 0.8313\n",
            "Epoch 9/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4943 - accuracy: 0.8574 - val_loss: 0.5730 - val_accuracy: 0.8326\n",
            "Epoch 10/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4815 - accuracy: 0.8608 - val_loss: 0.5590 - val_accuracy: 0.8377\n",
            "Epoch 11/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4704 - accuracy: 0.8632 - val_loss: 0.5486 - val_accuracy: 0.8393\n",
            "Epoch 12/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.4611 - accuracy: 0.8651 - val_loss: 0.5385 - val_accuracy: 0.8418\n",
            "Epoch 13/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4512 - accuracy: 0.8675 - val_loss: 0.5333 - val_accuracy: 0.8433\n",
            "Epoch 14/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.4419 - accuracy: 0.8700 - val_loss: 0.5245 - val_accuracy: 0.8453\n",
            "Epoch 15/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4333 - accuracy: 0.8729 - val_loss: 0.5158 - val_accuracy: 0.8497\n",
            "Epoch 16/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4253 - accuracy: 0.8750 - val_loss: 0.5066 - val_accuracy: 0.8521\n",
            "Epoch 17/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4169 - accuracy: 0.8777 - val_loss: 0.4993 - val_accuracy: 0.8553\n",
            "Epoch 18/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4087 - accuracy: 0.8801 - val_loss: 0.4945 - val_accuracy: 0.8566\n",
            "Epoch 19/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.4005 - accuracy: 0.8826 - val_loss: 0.4874 - val_accuracy: 0.8584\n",
            "Epoch 20/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3937 - accuracy: 0.8846 - val_loss: 0.4826 - val_accuracy: 0.8598\n",
            "Epoch 21/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3852 - accuracy: 0.8870 - val_loss: 0.4804 - val_accuracy: 0.8607\n",
            "Epoch 22/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3784 - accuracy: 0.8889 - val_loss: 0.4698 - val_accuracy: 0.8644\n",
            "Epoch 23/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3713 - accuracy: 0.8910 - val_loss: 0.4655 - val_accuracy: 0.8662\n",
            "Epoch 24/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3641 - accuracy: 0.8932 - val_loss: 0.4619 - val_accuracy: 0.8670\n",
            "Epoch 25/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3570 - accuracy: 0.8956 - val_loss: 0.4555 - val_accuracy: 0.8676\n",
            "Epoch 26/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3504 - accuracy: 0.8975 - val_loss: 0.4549 - val_accuracy: 0.8686\n",
            "Epoch 27/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3432 - accuracy: 0.9001 - val_loss: 0.4497 - val_accuracy: 0.8711\n",
            "Epoch 28/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3367 - accuracy: 0.9016 - val_loss: 0.4471 - val_accuracy: 0.8723\n",
            "Epoch 29/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3292 - accuracy: 0.9042 - val_loss: 0.4406 - val_accuracy: 0.8751\n",
            "Epoch 30/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3235 - accuracy: 0.9059 - val_loss: 0.4383 - val_accuracy: 0.8753\n",
            "Epoch 31/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3159 - accuracy: 0.9082 - val_loss: 0.4368 - val_accuracy: 0.8760\n",
            "Epoch 32/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3100 - accuracy: 0.9099 - val_loss: 0.4337 - val_accuracy: 0.8770\n",
            "Epoch 33/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.3036 - accuracy: 0.9112 - val_loss: 0.4298 - val_accuracy: 0.8786\n",
            "Epoch 34/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2972 - accuracy: 0.9132 - val_loss: 0.4307 - val_accuracy: 0.8776\n",
            "Epoch 35/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2905 - accuracy: 0.9153 - val_loss: 0.4267 - val_accuracy: 0.8791\n",
            "Epoch 36/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2843 - accuracy: 0.9168 - val_loss: 0.4235 - val_accuracy: 0.8806\n",
            "Epoch 37/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2783 - accuracy: 0.9186 - val_loss: 0.4246 - val_accuracy: 0.8803\n",
            "Epoch 38/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2718 - accuracy: 0.9206 - val_loss: 0.4206 - val_accuracy: 0.8811\n",
            "Epoch 39/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2665 - accuracy: 0.9220 - val_loss: 0.4205 - val_accuracy: 0.8807\n",
            "Epoch 40/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2613 - accuracy: 0.9234 - val_loss: 0.4192 - val_accuracy: 0.8826\n",
            "Epoch 41/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2552 - accuracy: 0.9251 - val_loss: 0.4184 - val_accuracy: 0.8830\n",
            "Epoch 42/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2494 - accuracy: 0.9268 - val_loss: 0.4189 - val_accuracy: 0.8832\n",
            "Epoch 43/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2446 - accuracy: 0.9281 - val_loss: 0.4168 - val_accuracy: 0.8835\n",
            "Epoch 44/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2387 - accuracy: 0.9299 - val_loss: 0.4176 - val_accuracy: 0.8840\n",
            "Epoch 45/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2475 - accuracy: 0.9270 - val_loss: 0.4116 - val_accuracy: 0.8854\n",
            "Epoch 46/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2329 - accuracy: 0.9312 - val_loss: 0.4142 - val_accuracy: 0.8856\n",
            "Epoch 47/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2262 - accuracy: 0.9335 - val_loss: 0.4172 - val_accuracy: 0.8857\n",
            "Epoch 48/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2198 - accuracy: 0.9355 - val_loss: 0.4185 - val_accuracy: 0.8858\n",
            "Epoch 49/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2150 - accuracy: 0.9366 - val_loss: 0.4191 - val_accuracy: 0.8857\n",
            "Epoch 50/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2100 - accuracy: 0.9381 - val_loss: 0.4208 - val_accuracy: 0.8862\n",
            "Epoch 51/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.2051 - accuracy: 0.9395 - val_loss: 0.4227 - val_accuracy: 0.8862\n",
            "Epoch 52/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1999 - accuracy: 0.9409 - val_loss: 0.4230 - val_accuracy: 0.8868\n",
            "Epoch 53/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1956 - accuracy: 0.9422 - val_loss: 0.4233 - val_accuracy: 0.8873\n",
            "Epoch 54/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.1909 - accuracy: 0.9437 - val_loss: 0.4269 - val_accuracy: 0.8858\n",
            "Epoch 55/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1859 - accuracy: 0.9452 - val_loss: 0.4299 - val_accuracy: 0.8865\n",
            "Epoch 56/60\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.1829 - accuracy: 0.9458 - val_loss: 0.4284 - val_accuracy: 0.8876\n",
            "Epoch 57/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1776 - accuracy: 0.9472 - val_loss: 0.4335 - val_accuracy: 0.8875\n",
            "Epoch 58/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1727 - accuracy: 0.9487 - val_loss: 0.4357 - val_accuracy: 0.8871\n",
            "Epoch 59/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1684 - accuracy: 0.9499 - val_loss: 0.4372 - val_accuracy: 0.8877\n",
            "Epoch 60/60\n",
            "125/125 [==============================] - 4s 33ms/step - loss: 0.1644 - accuracy: 0.9509 - val_loss: 0.4378 - val_accuracy: 0.8883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jg7egOb5KUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e5ea3f4f-780e-435f-8eef-ee8d8615a3a2"
      },
      "source": [
        "\n",
        "for seq_index in range(100):\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите сюда.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите сюда.\n",
            "\n",
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Идите сюда.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Можновой?\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Можновой?\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Можновой?\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Можновой?\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Можновой?\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Бегите!\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Бегите!\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Бегите его.\n",
            "\n",
            "-\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Бегите его.\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Кто такой кол?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Вот одано!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Пожал!\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Пожал!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Помогите!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Помогите!\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: Помогите!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Прыгайте!\n",
            "\n",
            "-\n",
            "Input sentence: Jump!\n",
            "Decoded sentence: Прыгайте!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Прыгайте!\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Прыгайте!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Подождите!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Подождите!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Подождите!\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Подождите!\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Подождите.\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Подождите.\n",
            "\n",
            "-\n",
            "Input sentence: Wait.\n",
            "Decoded sentence: Подождите.\n",
            "\n",
            "-\n",
            "Input sentence: Do it.\n",
            "Decoded sentence: Сделай это.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Идите на мне.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Идите на мне.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Привет!\n",
            "\n",
            "-\n",
            "Input sentence: Hurry!\n",
            "Decoded sentence: Будь меня!\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я её помол.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я её помол.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я её помол.\n",
            "\n",
            "-\n",
            "Input sentence: I ran.\n",
            "Decoded sentence: Я её помол.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Я выжул.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Я выжул.\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Я выжул.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я помогала.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я помогала.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: Я помогала.\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Я победила!\n",
            "\n",
            "-\n",
            "Input sentence: Oh no!\n",
            "Decoded sentence: Оно позвал?\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Скажите ещё.\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Скажите ещё.\n",
            "\n",
            "-\n",
            "Input sentence: Relax.\n",
            "Decoded sentence: Скажите ещё.\n",
            "\n",
            "-\n",
            "Input sentence: Shoot!\n",
            "Decoded sentence: Потормутесь!\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Smile.\n",
            "Decoded sentence: Улыбнитесь.\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Остановитесь!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За нешело!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За нешело!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За нешело!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За нешело!\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: За нешело!\n",
            "\n",
            "-\n",
            "Input sentence: Eat it.\n",
            "Decoded sentence: Поешьте сомоть.\n",
            "\n",
            "-\n",
            "Input sentence: Eat up.\n",
            "Decoded sentence: Идите садаться.\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Ни о мезно меняеть?\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Ни о мезно меняеть?\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Ни о мезно меняеть?\n",
            "\n",
            "-\n",
            "Input sentence: Freeze!\n",
            "Decoded sentence: Ни о мезно меняеть?\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Поднимайся.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Идите ид.\n",
            "\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Послыбайте!\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Поелашайте?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Поелашайте?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Поелашайте?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Поелашайте?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aMowyQj7WYj",
        "colab_type": "text"
      },
      "source": [
        "В общем и целом, неплохо, но ровно до тех пор, пока сеть не начинает выдавать несуществующие слова. Но это вполне объяснимо."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srpebzFL3khe",
        "colab_type": "text"
      },
      "source": [
        "## Спускаемся на уровень слов и используем Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vQTe0eN2tOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "#tf.enable_eager_execution()\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Zа-яА-ЯёЁ?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(preprocess_sentence(input_text))\n",
        "    target_texts.append(preprocess_sentence(target_text))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZsZMQw71RE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8a3a02e-a76f-4d6f-d43c-8dfe26b87479"
      },
      "source": [
        "input_texts[0], target_texts[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<start> Go . <end>', '<start> Марш ! <end>')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9IDwJhx3C2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgLCMIrA3I0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
        "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlGGF7jf3I5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73yckIoV3I38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLzJAi73Ix2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.lstm(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "    \n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "    \n",
        "    \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.lstm(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb8khQFs8YQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXfAp8FE8YSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "    \n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGJkTvxU8YXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "4e6cfd8e-ca90-4c63-e89c-43cca636da1a"
      },
      "source": [
        "EPOCHS = 50\n",
        "for epoch in range(EPOCHS):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "    \n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 1.6201\n",
            "Epoch 2 Loss 1.2245\n",
            "Epoch 3 Loss 1.0615\n",
            "Epoch 4 Loss 0.9248\n",
            "Epoch 5 Loss 0.8063\n",
            "Epoch 6 Loss 0.6947\n",
            "Epoch 7 Loss 0.6002\n",
            "Epoch 8 Loss 0.5114\n",
            "Epoch 9 Loss 0.4298\n",
            "Epoch 10 Loss 0.3610\n",
            "Epoch 11 Loss 0.3053\n",
            "Epoch 12 Loss 0.2625\n",
            "Epoch 13 Loss 0.2318\n",
            "Epoch 14 Loss 0.2077\n",
            "Epoch 15 Loss 0.1887\n",
            "Epoch 16 Loss 0.1768\n",
            "Epoch 17 Loss 0.1675\n",
            "Epoch 18 Loss 0.1605\n",
            "Epoch 19 Loss 0.1559\n",
            "Epoch 20 Loss 0.1500\n",
            "Epoch 21 Loss 0.1460\n",
            "Epoch 22 Loss 0.1439\n",
            "Epoch 23 Loss 0.1408\n",
            "Epoch 24 Loss 0.1372\n",
            "Epoch 25 Loss 0.1362\n",
            "Epoch 26 Loss 0.1325\n",
            "Epoch 27 Loss 0.1305\n",
            "Epoch 28 Loss 0.1285\n",
            "Epoch 29 Loss 0.1286\n",
            "Epoch 30 Loss 0.1269\n",
            "Epoch 31 Loss 0.1256\n",
            "Epoch 32 Loss 0.1249\n",
            "Epoch 33 Loss 0.1232\n",
            "Epoch 34 Loss 0.1253\n",
            "Epoch 35 Loss 0.1235\n",
            "Epoch 36 Loss 0.1234\n",
            "Epoch 37 Loss 0.1205\n",
            "Epoch 38 Loss 0.1192\n",
            "Epoch 39 Loss 0.1159\n",
            "Epoch 40 Loss 0.1161\n",
            "Epoch 41 Loss 0.1153\n",
            "Epoch 42 Loss 0.1132\n",
            "Epoch 43 Loss 0.1134\n",
            "Epoch 44 Loss 0.1130\n",
            "Epoch 45 Loss 0.1121\n",
            "Epoch 46 Loss 0.1123\n",
            "Epoch 47 Loss 0.1114\n",
            "Epoch 48 Loss 0.1107\n",
            "Epoch 49 Loss 0.1108\n",
            "Epoch 50 Loss 0.1113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jug6wMM9exJ",
        "colab_type": "text"
      },
      "source": [
        "Воруем ворованные функции для оценки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oko1sNcX_t8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_3FRK7c8YOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    fontdict = {'fontsize': 14}\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    plt.show()\n",
        "    \n",
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZbscXnE9lKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "2a54d28b-1567-429d-c88f-599b27a65304"
      },
      "source": [
        "translate(u'keep calm')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> keep calm <end>\n",
            "Predicted translation: сохраняйте спокойствие . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MoPMFzUArhV",
        "colab_type": "text"
      },
      "source": [
        "Пока адекватно"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDGAbsLJA3Yk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "98c880bf-ba5c-4f48-a17c-24db32a10b17"
      },
      "source": [
        "translate(u'good morning')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> good morning <end>\n",
            "Predicted translation: сохраняйте спокойствие . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaEnqhocA3Vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "3c900a96-9972-436a-d74c-baabb397755a"
      },
      "source": [
        "translate(u'have a nice day')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> have a nice day <end>\n",
            "Predicted translation: время истекло . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj5gnRdKA-8V",
        "colab_type": "text"
      },
      "source": [
        "А вот теперь вообще неадекватный перевод"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcPUd3pBBBaL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "4f135d87-7725-40f8-ed99-0cabab804fe8"
      },
      "source": [
        "translate(u'hold on')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hold on <end>\n",
            "Predicted translation: не кладите трубку . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuh3h8JDCIkY",
        "colab_type": "text"
      },
      "source": [
        "В общем, порой и довольно часто, перевод адекватный, но бывают и проблемы. Из плюсов - сеть не выдаёт несуществующих слов, что логично, т.к. обучение идёт на уровне слов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AeAmgoVCUHP",
        "colab_type": "text"
      },
      "source": [
        "## Пробуем трансформер\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COtA2SWm9lHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR-KCvAUCxLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RGwjUUuCzZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(\n",
        "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "    \n",
        "    \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                self.d_model)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImLSQTpyC2Ii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                                 look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UnAcZArC4VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                               input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                               target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15oRkiwcC5x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "\n",
        "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
        "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxwY3T77DG_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftG5zrkUDG2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "  \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0P-nGPkDLhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBYbmtJpDLSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1C_mHNsD2lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m22r760kD2Ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "  \n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "    train_loss(loss)\n",
        "    train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1vLMyQ9D4Q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "394a959a-95f8-4fe8-84d5-ebc1b6ab778d"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        train_step(inp, tar)\n",
        "        if batch % 50 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 8.4983 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 8.3160 Accuracy 0.0310\n",
            "Epoch 1 Batch 100 Loss 8.0585 Accuracy 0.0607\n",
            "Epoch 1 Loss 7.9634 Accuracy 0.0665\n",
            "Epoch 2 Batch 0 Loss 7.4937 Accuracy 0.0909\n",
            "Epoch 2 Batch 50 Loss 7.3037 Accuracy 0.0909\n",
            "Epoch 2 Batch 100 Loss 7.0655 Accuracy 0.1096\n",
            "Epoch 2 Loss 6.9422 Accuracy 0.1199\n",
            "Epoch 3 Batch 0 Loss 6.1829 Accuracy 0.1690\n",
            "Epoch 3 Batch 50 Loss 5.9579 Accuracy 0.1690\n",
            "Epoch 3 Batch 100 Loss 5.6838 Accuracy 0.1752\n",
            "Epoch 3 Loss 5.5525 Accuracy 0.1776\n",
            "Epoch 4 Batch 0 Loss 4.7699 Accuracy 0.1918\n",
            "Epoch 4 Batch 50 Loss 4.5565 Accuracy 0.1965\n",
            "Epoch 4 Batch 100 Loss 4.3661 Accuracy 0.2008\n",
            "Epoch 4 Loss 4.2834 Accuracy 0.2023\n",
            "Epoch 5 Batch 0 Loss 3.5916 Accuracy 0.2145\n",
            "Epoch 5 Batch 50 Loss 3.6910 Accuracy 0.2114\n",
            "Epoch 5 Batch 100 Loss 3.6071 Accuracy 0.2145\n",
            "Epoch 5 Loss 3.5732 Accuracy 0.2163\n",
            "Epoch 6 Batch 0 Loss 3.2966 Accuracy 0.2344\n",
            "Epoch 6 Batch 50 Loss 3.2874 Accuracy 0.2277\n",
            "Epoch 6 Batch 100 Loss 3.2267 Accuracy 0.2296\n",
            "Epoch 6 Loss 3.1964 Accuracy 0.2300\n",
            "Epoch 7 Batch 0 Loss 2.9378 Accuracy 0.2401\n",
            "Epoch 7 Batch 50 Loss 3.0006 Accuracy 0.2342\n",
            "Epoch 7 Batch 100 Loss 2.9658 Accuracy 0.2358\n",
            "Epoch 7 Loss 2.9548 Accuracy 0.2360\n",
            "Epoch 8 Batch 0 Loss 2.8913 Accuracy 0.2429\n",
            "Epoch 8 Batch 50 Loss 2.8006 Accuracy 0.2404\n",
            "Epoch 8 Batch 100 Loss 2.7724 Accuracy 0.2414\n",
            "Epoch 8 Loss 2.7647 Accuracy 0.2418\n",
            "Epoch 9 Batch 0 Loss 2.4600 Accuracy 0.2628\n",
            "Epoch 9 Batch 50 Loss 2.6018 Accuracy 0.2457\n",
            "Epoch 9 Batch 100 Loss 2.5986 Accuracy 0.2470\n",
            "Epoch 9 Loss 2.5982 Accuracy 0.2467\n",
            "Epoch 10 Batch 0 Loss 2.4197 Accuracy 0.2443\n",
            "Epoch 10 Batch 50 Loss 2.4843 Accuracy 0.2496\n",
            "Epoch 10 Batch 100 Loss 2.4590 Accuracy 0.2505\n",
            "Epoch 10 Loss 2.4541 Accuracy 0.2508\n",
            "Epoch 11 Batch 0 Loss 2.2500 Accuracy 0.2543\n",
            "Epoch 11 Batch 50 Loss 2.3211 Accuracy 0.2545\n",
            "Epoch 11 Batch 100 Loss 2.3094 Accuracy 0.2550\n",
            "Epoch 11 Loss 2.3117 Accuracy 0.2554\n",
            "Epoch 12 Batch 0 Loss 2.1159 Accuracy 0.2741\n",
            "Epoch 12 Batch 50 Loss 2.1710 Accuracy 0.2594\n",
            "Epoch 12 Batch 100 Loss 2.1707 Accuracy 0.2598\n",
            "Epoch 12 Loss 2.1772 Accuracy 0.2595\n",
            "Epoch 13 Batch 0 Loss 1.9640 Accuracy 0.2812\n",
            "Epoch 13 Batch 50 Loss 2.0165 Accuracy 0.2643\n",
            "Epoch 13 Batch 100 Loss 2.0425 Accuracy 0.2633\n",
            "Epoch 13 Loss 2.0411 Accuracy 0.2638\n",
            "Epoch 14 Batch 0 Loss 1.8272 Accuracy 0.2855\n",
            "Epoch 14 Batch 50 Loss 1.8870 Accuracy 0.2715\n",
            "Epoch 14 Batch 100 Loss 1.9048 Accuracy 0.2694\n",
            "Epoch 14 Loss 1.9092 Accuracy 0.2688\n",
            "Epoch 15 Batch 0 Loss 1.6820 Accuracy 0.2756\n",
            "Epoch 15 Batch 50 Loss 1.7686 Accuracy 0.2728\n",
            "Epoch 15 Batch 100 Loss 1.7860 Accuracy 0.2720\n",
            "Epoch 15 Loss 1.7898 Accuracy 0.2722\n",
            "Epoch 16 Batch 0 Loss 1.6484 Accuracy 0.2685\n",
            "Epoch 16 Batch 50 Loss 1.6675 Accuracy 0.2771\n",
            "Epoch 16 Batch 100 Loss 1.6671 Accuracy 0.2779\n",
            "Epoch 16 Loss 1.6663 Accuracy 0.2779\n",
            "Epoch 17 Batch 0 Loss 1.4665 Accuracy 0.2898\n",
            "Epoch 17 Batch 50 Loss 1.5201 Accuracy 0.2839\n",
            "Epoch 17 Batch 100 Loss 1.5397 Accuracy 0.2835\n",
            "Epoch 17 Loss 1.5461 Accuracy 0.2830\n",
            "Epoch 18 Batch 0 Loss 1.2576 Accuracy 0.3097\n",
            "Epoch 18 Batch 50 Loss 1.3863 Accuracy 0.2917\n",
            "Epoch 18 Batch 100 Loss 1.4159 Accuracy 0.2899\n",
            "Epoch 18 Loss 1.4242 Accuracy 0.2893\n",
            "Epoch 19 Batch 0 Loss 1.2706 Accuracy 0.3125\n",
            "Epoch 19 Batch 50 Loss 1.2837 Accuracy 0.2967\n",
            "Epoch 19 Batch 100 Loss 1.3074 Accuracy 0.2943\n",
            "Epoch 19 Loss 1.3176 Accuracy 0.2934\n",
            "Epoch 20 Batch 0 Loss 1.2043 Accuracy 0.3054\n",
            "Epoch 20 Batch 50 Loss 1.1666 Accuracy 0.3024\n",
            "Epoch 20 Batch 100 Loss 1.1901 Accuracy 0.3004\n",
            "Epoch 20 Loss 1.2078 Accuracy 0.2999\n",
            "Epoch 21 Batch 0 Loss 1.0997 Accuracy 0.3153\n",
            "Epoch 21 Batch 50 Loss 1.0615 Accuracy 0.3075\n",
            "Epoch 21 Batch 100 Loss 1.1019 Accuracy 0.3041\n",
            "Epoch 21 Loss 1.1181 Accuracy 0.3036\n",
            "Epoch 22 Batch 0 Loss 0.9870 Accuracy 0.3082\n",
            "Epoch 22 Batch 50 Loss 0.9532 Accuracy 0.3148\n",
            "Epoch 22 Batch 100 Loss 0.9989 Accuracy 0.3110\n",
            "Epoch 22 Loss 1.0207 Accuracy 0.3099\n",
            "Epoch 23 Batch 0 Loss 0.8528 Accuracy 0.3239\n",
            "Epoch 23 Batch 50 Loss 0.9028 Accuracy 0.3180\n",
            "Epoch 23 Batch 100 Loss 0.9452 Accuracy 0.3136\n",
            "Epoch 23 Loss 0.9606 Accuracy 0.3119\n",
            "Epoch 24 Batch 0 Loss 0.7415 Accuracy 0.3281\n",
            "Epoch 24 Batch 50 Loss 0.8171 Accuracy 0.3225\n",
            "Epoch 24 Batch 100 Loss 0.8718 Accuracy 0.3163\n",
            "Epoch 24 Loss 0.8922 Accuracy 0.3149\n",
            "Epoch 25 Batch 0 Loss 0.8016 Accuracy 0.3267\n",
            "Epoch 25 Batch 50 Loss 0.7632 Accuracy 0.3250\n",
            "Epoch 25 Batch 100 Loss 0.8055 Accuracy 0.3200\n",
            "Epoch 25 Loss 0.8280 Accuracy 0.3184\n",
            "Epoch 26 Batch 0 Loss 0.8090 Accuracy 0.3224\n",
            "Epoch 26 Batch 50 Loss 0.7085 Accuracy 0.3302\n",
            "Epoch 26 Batch 100 Loss 0.7662 Accuracy 0.3233\n",
            "Epoch 26 Loss 0.7840 Accuracy 0.3215\n",
            "Epoch 27 Batch 0 Loss 0.5746 Accuracy 0.3523\n",
            "Epoch 27 Batch 50 Loss 0.6803 Accuracy 0.3305\n",
            "Epoch 27 Batch 100 Loss 0.7295 Accuracy 0.3262\n",
            "Epoch 27 Loss 0.7495 Accuracy 0.3239\n",
            "Epoch 28 Batch 0 Loss 0.5909 Accuracy 0.3409\n",
            "Epoch 28 Batch 50 Loss 0.6261 Accuracy 0.3355\n",
            "Epoch 28 Batch 100 Loss 0.6913 Accuracy 0.3284\n",
            "Epoch 28 Loss 0.7134 Accuracy 0.3259\n",
            "Epoch 29 Batch 0 Loss 0.5530 Accuracy 0.3423\n",
            "Epoch 29 Batch 50 Loss 0.6231 Accuracy 0.3353\n",
            "Epoch 29 Batch 100 Loss 0.6724 Accuracy 0.3303\n",
            "Epoch 29 Loss 0.6941 Accuracy 0.3275\n",
            "Epoch 30 Batch 0 Loss 0.6070 Accuracy 0.3253\n",
            "Epoch 30 Batch 50 Loss 0.6124 Accuracy 0.3360\n",
            "Epoch 30 Batch 100 Loss 0.6601 Accuracy 0.3303\n",
            "Epoch 30 Loss 0.6793 Accuracy 0.3284\n",
            "Epoch 31 Batch 0 Loss 0.5521 Accuracy 0.3480\n",
            "Epoch 31 Batch 50 Loss 0.5687 Accuracy 0.3388\n",
            "Epoch 31 Batch 100 Loss 0.6275 Accuracy 0.3338\n",
            "Epoch 31 Loss 0.6525 Accuracy 0.3316\n",
            "Epoch 32 Batch 0 Loss 0.6231 Accuracy 0.3665\n",
            "Epoch 32 Batch 50 Loss 0.5760 Accuracy 0.3369\n",
            "Epoch 32 Batch 100 Loss 0.6373 Accuracy 0.3322\n",
            "Epoch 32 Loss 0.6570 Accuracy 0.3302\n",
            "Epoch 33 Batch 0 Loss 0.4841 Accuracy 0.3438\n",
            "Epoch 33 Batch 50 Loss 0.5705 Accuracy 0.3383\n",
            "Epoch 33 Batch 100 Loss 0.6231 Accuracy 0.3330\n",
            "Epoch 33 Loss 0.6390 Accuracy 0.3317\n",
            "Epoch 34 Batch 0 Loss 0.5548 Accuracy 0.3338\n",
            "Epoch 34 Batch 50 Loss 0.5439 Accuracy 0.3407\n",
            "Epoch 34 Batch 100 Loss 0.5983 Accuracy 0.3356\n",
            "Epoch 34 Loss 0.6167 Accuracy 0.3344\n",
            "Epoch 35 Batch 0 Loss 0.5057 Accuracy 0.3452\n",
            "Epoch 35 Batch 50 Loss 0.5357 Accuracy 0.3422\n",
            "Epoch 35 Batch 100 Loss 0.5809 Accuracy 0.3380\n",
            "Epoch 35 Loss 0.6010 Accuracy 0.3359\n",
            "Epoch 36 Batch 0 Loss 0.4591 Accuracy 0.3509\n",
            "Epoch 36 Batch 50 Loss 0.5130 Accuracy 0.3450\n",
            "Epoch 36 Batch 100 Loss 0.5686 Accuracy 0.3382\n",
            "Epoch 36 Loss 0.5914 Accuracy 0.3368\n",
            "Epoch 37 Batch 0 Loss 0.4451 Accuracy 0.3480\n",
            "Epoch 37 Batch 50 Loss 0.5105 Accuracy 0.3428\n",
            "Epoch 37 Batch 100 Loss 0.5525 Accuracy 0.3395\n",
            "Epoch 37 Loss 0.5657 Accuracy 0.3382\n",
            "Epoch 38 Batch 0 Loss 0.4222 Accuracy 0.3594\n",
            "Epoch 38 Batch 50 Loss 0.4791 Accuracy 0.3467\n",
            "Epoch 38 Batch 100 Loss 0.5335 Accuracy 0.3420\n",
            "Epoch 38 Loss 0.5543 Accuracy 0.3400\n",
            "Epoch 39 Batch 0 Loss 0.5272 Accuracy 0.3324\n",
            "Epoch 39 Batch 50 Loss 0.4863 Accuracy 0.3446\n",
            "Epoch 39 Batch 100 Loss 0.5178 Accuracy 0.3426\n",
            "Epoch 39 Loss 0.5385 Accuracy 0.3408\n",
            "Epoch 40 Batch 0 Loss 0.4556 Accuracy 0.3551\n",
            "Epoch 40 Batch 50 Loss 0.4685 Accuracy 0.3482\n",
            "Epoch 40 Batch 100 Loss 0.5203 Accuracy 0.3427\n",
            "Epoch 40 Loss 0.5379 Accuracy 0.3411\n",
            "Epoch 41 Batch 0 Loss 0.4033 Accuracy 0.3423\n",
            "Epoch 41 Batch 50 Loss 0.4667 Accuracy 0.3475\n",
            "Epoch 41 Batch 100 Loss 0.5066 Accuracy 0.3434\n",
            "Epoch 41 Loss 0.5285 Accuracy 0.3413\n",
            "Epoch 42 Batch 0 Loss 0.3533 Accuracy 0.3580\n",
            "Epoch 42 Batch 50 Loss 0.4454 Accuracy 0.3476\n",
            "Epoch 42 Batch 100 Loss 0.5004 Accuracy 0.3432\n",
            "Epoch 42 Loss 0.5134 Accuracy 0.3429\n",
            "Epoch 43 Batch 0 Loss 0.4091 Accuracy 0.3651\n",
            "Epoch 43 Batch 50 Loss 0.4260 Accuracy 0.3518\n",
            "Epoch 43 Batch 100 Loss 0.4744 Accuracy 0.3465\n",
            "Epoch 43 Loss 0.4945 Accuracy 0.3449\n",
            "Epoch 44 Batch 0 Loss 0.3561 Accuracy 0.3594\n",
            "Epoch 44 Batch 50 Loss 0.4302 Accuracy 0.3534\n",
            "Epoch 44 Batch 100 Loss 0.4817 Accuracy 0.3468\n",
            "Epoch 44 Loss 0.4980 Accuracy 0.3451\n",
            "Epoch 45 Batch 0 Loss 0.4336 Accuracy 0.3438\n",
            "Epoch 45 Batch 50 Loss 0.4275 Accuracy 0.3526\n",
            "Epoch 45 Batch 100 Loss 0.4754 Accuracy 0.3472\n",
            "Epoch 45 Loss 0.4933 Accuracy 0.3453\n",
            "Epoch 46 Batch 0 Loss 0.4135 Accuracy 0.3409\n",
            "Epoch 46 Batch 50 Loss 0.4281 Accuracy 0.3510\n",
            "Epoch 46 Batch 100 Loss 0.4646 Accuracy 0.3478\n",
            "Epoch 46 Loss 0.4772 Accuracy 0.3463\n",
            "Epoch 47 Batch 0 Loss 0.3560 Accuracy 0.3736\n",
            "Epoch 47 Batch 50 Loss 0.4139 Accuracy 0.3524\n",
            "Epoch 47 Batch 100 Loss 0.4558 Accuracy 0.3488\n",
            "Epoch 47 Loss 0.4725 Accuracy 0.3474\n",
            "Epoch 48 Batch 0 Loss 0.3134 Accuracy 0.3622\n",
            "Epoch 48 Batch 50 Loss 0.4067 Accuracy 0.3536\n",
            "Epoch 48 Batch 100 Loss 0.4495 Accuracy 0.3486\n",
            "Epoch 48 Loss 0.4683 Accuracy 0.3473\n",
            "Epoch 49 Batch 0 Loss 0.3124 Accuracy 0.3537\n",
            "Epoch 49 Batch 50 Loss 0.4054 Accuracy 0.3529\n",
            "Epoch 49 Batch 100 Loss 0.4518 Accuracy 0.3475\n",
            "Epoch 49 Loss 0.4690 Accuracy 0.3465\n",
            "Epoch 50 Batch 0 Loss 0.3280 Accuracy 0.3679\n",
            "Epoch 50 Batch 50 Loss 0.3880 Accuracy 0.3555\n",
            "Epoch 50 Batch 100 Loss 0.4349 Accuracy 0.3505\n",
            "Epoch 50 Loss 0.4568 Accuracy 0.3481\n",
            "Epoch 51 Batch 0 Loss 0.3691 Accuracy 0.3679\n",
            "Epoch 51 Batch 50 Loss 0.3919 Accuracy 0.3532\n",
            "Epoch 51 Batch 100 Loss 0.4293 Accuracy 0.3506\n",
            "Epoch 51 Loss 0.4450 Accuracy 0.3489\n",
            "Epoch 52 Batch 0 Loss 0.3299 Accuracy 0.3494\n",
            "Epoch 52 Batch 50 Loss 0.3920 Accuracy 0.3548\n",
            "Epoch 52 Batch 100 Loss 0.4257 Accuracy 0.3504\n",
            "Epoch 52 Loss 0.4425 Accuracy 0.3490\n",
            "Epoch 53 Batch 0 Loss 0.3768 Accuracy 0.3438\n",
            "Epoch 53 Batch 50 Loss 0.3826 Accuracy 0.3556\n",
            "Epoch 53 Batch 100 Loss 0.4281 Accuracy 0.3503\n",
            "Epoch 53 Loss 0.4437 Accuracy 0.3492\n",
            "Epoch 54 Batch 0 Loss 0.3720 Accuracy 0.3523\n",
            "Epoch 54 Batch 50 Loss 0.3780 Accuracy 0.3565\n",
            "Epoch 54 Batch 100 Loss 0.4188 Accuracy 0.3520\n",
            "Epoch 54 Loss 0.4353 Accuracy 0.3505\n",
            "Epoch 55 Batch 0 Loss 0.3517 Accuracy 0.3509\n",
            "Epoch 55 Batch 50 Loss 0.3725 Accuracy 0.3563\n",
            "Epoch 55 Batch 100 Loss 0.4119 Accuracy 0.3530\n",
            "Epoch 55 Loss 0.4306 Accuracy 0.3508\n",
            "Epoch 56 Batch 0 Loss 0.2814 Accuracy 0.3736\n",
            "Epoch 56 Batch 50 Loss 0.3749 Accuracy 0.3544\n",
            "Epoch 56 Batch 100 Loss 0.4128 Accuracy 0.3517\n",
            "Epoch 56 Loss 0.4296 Accuracy 0.3502\n",
            "Epoch 57 Batch 0 Loss 0.3117 Accuracy 0.3679\n",
            "Epoch 57 Batch 50 Loss 0.3653 Accuracy 0.3575\n",
            "Epoch 57 Batch 100 Loss 0.4109 Accuracy 0.3526\n",
            "Epoch 57 Loss 0.4230 Accuracy 0.3514\n",
            "Epoch 58 Batch 0 Loss 0.3420 Accuracy 0.3494\n",
            "Epoch 58 Batch 50 Loss 0.3680 Accuracy 0.3556\n",
            "Epoch 58 Batch 100 Loss 0.4044 Accuracy 0.3522\n",
            "Epoch 58 Loss 0.4218 Accuracy 0.3506\n",
            "Epoch 59 Batch 0 Loss 0.2863 Accuracy 0.3707\n",
            "Epoch 59 Batch 50 Loss 0.3642 Accuracy 0.3562\n",
            "Epoch 59 Batch 100 Loss 0.4042 Accuracy 0.3526\n",
            "Epoch 59 Loss 0.4211 Accuracy 0.3511\n",
            "Epoch 60 Batch 0 Loss 0.3233 Accuracy 0.3565\n",
            "Epoch 60 Batch 50 Loss 0.3578 Accuracy 0.3558\n",
            "Epoch 60 Batch 100 Loss 0.3984 Accuracy 0.3528\n",
            "Epoch 60 Loss 0.4147 Accuracy 0.3514\n",
            "Epoch 61 Batch 0 Loss 0.3266 Accuracy 0.3608\n",
            "Epoch 61 Batch 50 Loss 0.3505 Accuracy 0.3566\n",
            "Epoch 61 Batch 100 Loss 0.3916 Accuracy 0.3530\n",
            "Epoch 61 Loss 0.4093 Accuracy 0.3517\n",
            "Epoch 62 Batch 0 Loss 0.2569 Accuracy 0.3736\n",
            "Epoch 62 Batch 50 Loss 0.3456 Accuracy 0.3596\n",
            "Epoch 62 Batch 100 Loss 0.3932 Accuracy 0.3535\n",
            "Epoch 62 Loss 0.4103 Accuracy 0.3515\n",
            "Epoch 63 Batch 0 Loss 0.2907 Accuracy 0.3665\n",
            "Epoch 63 Batch 50 Loss 0.3477 Accuracy 0.3573\n",
            "Epoch 63 Batch 100 Loss 0.3891 Accuracy 0.3534\n",
            "Epoch 63 Loss 0.4038 Accuracy 0.3523\n",
            "Epoch 64 Batch 0 Loss 0.3017 Accuracy 0.3509\n",
            "Epoch 64 Batch 50 Loss 0.3454 Accuracy 0.3583\n",
            "Epoch 64 Batch 100 Loss 0.3855 Accuracy 0.3536\n",
            "Epoch 64 Loss 0.4005 Accuracy 0.3518\n",
            "Epoch 65 Batch 0 Loss 0.3135 Accuracy 0.3707\n",
            "Epoch 65 Batch 50 Loss 0.3437 Accuracy 0.3587\n",
            "Epoch 65 Batch 100 Loss 0.3834 Accuracy 0.3533\n",
            "Epoch 65 Loss 0.3976 Accuracy 0.3529\n",
            "Epoch 66 Batch 0 Loss 0.3253 Accuracy 0.3580\n",
            "Epoch 66 Batch 50 Loss 0.3456 Accuracy 0.3573\n",
            "Epoch 66 Batch 100 Loss 0.3886 Accuracy 0.3526\n",
            "Epoch 66 Loss 0.4011 Accuracy 0.3518\n",
            "Epoch 67 Batch 0 Loss 0.3257 Accuracy 0.3594\n",
            "Epoch 67 Batch 50 Loss 0.3350 Accuracy 0.3599\n",
            "Epoch 67 Batch 100 Loss 0.3745 Accuracy 0.3549\n",
            "Epoch 67 Loss 0.3886 Accuracy 0.3535\n",
            "Epoch 68 Batch 0 Loss 0.3352 Accuracy 0.3466\n",
            "Epoch 68 Batch 50 Loss 0.3369 Accuracy 0.3578\n",
            "Epoch 68 Batch 100 Loss 0.3735 Accuracy 0.3540\n",
            "Epoch 68 Loss 0.3917 Accuracy 0.3524\n",
            "Epoch 69 Batch 0 Loss 0.3362 Accuracy 0.3565\n",
            "Epoch 69 Batch 50 Loss 0.3206 Accuracy 0.3604\n",
            "Epoch 69 Batch 100 Loss 0.3670 Accuracy 0.3554\n",
            "Epoch 69 Loss 0.3862 Accuracy 0.3529\n",
            "Epoch 70 Batch 0 Loss 0.2984 Accuracy 0.3793\n",
            "Epoch 70 Batch 50 Loss 0.3414 Accuracy 0.3611\n",
            "Epoch 70 Batch 100 Loss 0.3730 Accuracy 0.3553\n",
            "Epoch 70 Loss 0.3898 Accuracy 0.3521\n",
            "Epoch 71 Batch 0 Loss 0.3649 Accuracy 0.3395\n",
            "Epoch 71 Batch 50 Loss 0.3266 Accuracy 0.3599\n",
            "Epoch 71 Batch 100 Loss 0.3655 Accuracy 0.3554\n",
            "Epoch 71 Loss 0.3818 Accuracy 0.3531\n",
            "Epoch 72 Batch 0 Loss 0.2373 Accuracy 0.3565\n",
            "Epoch 72 Batch 50 Loss 0.3332 Accuracy 0.3577\n",
            "Epoch 72 Batch 100 Loss 0.3686 Accuracy 0.3552\n",
            "Epoch 72 Loss 0.3839 Accuracy 0.3528\n",
            "Epoch 73 Batch 0 Loss 0.2413 Accuracy 0.3537\n",
            "Epoch 73 Batch 50 Loss 0.3379 Accuracy 0.3584\n",
            "Epoch 73 Batch 100 Loss 0.3636 Accuracy 0.3556\n",
            "Epoch 73 Loss 0.3777 Accuracy 0.3541\n",
            "Epoch 74 Batch 0 Loss 0.2247 Accuracy 0.3963\n",
            "Epoch 74 Batch 50 Loss 0.3265 Accuracy 0.3602\n",
            "Epoch 74 Batch 100 Loss 0.3645 Accuracy 0.3551\n",
            "Epoch 74 Loss 0.3798 Accuracy 0.3535\n",
            "Epoch 75 Batch 0 Loss 0.2785 Accuracy 0.3665\n",
            "Epoch 75 Batch 50 Loss 0.3208 Accuracy 0.3607\n",
            "Epoch 75 Batch 100 Loss 0.3563 Accuracy 0.3564\n",
            "Epoch 75 Loss 0.3713 Accuracy 0.3546\n",
            "Epoch 76 Batch 0 Loss 0.2444 Accuracy 0.3693\n",
            "Epoch 76 Batch 50 Loss 0.3227 Accuracy 0.3601\n",
            "Epoch 76 Batch 100 Loss 0.3581 Accuracy 0.3561\n",
            "Epoch 76 Loss 0.3735 Accuracy 0.3547\n",
            "Epoch 77 Batch 0 Loss 0.2270 Accuracy 0.3892\n",
            "Epoch 77 Batch 50 Loss 0.3181 Accuracy 0.3600\n",
            "Epoch 77 Batch 100 Loss 0.3543 Accuracy 0.3564\n",
            "Epoch 77 Loss 0.3728 Accuracy 0.3537\n",
            "Epoch 78 Batch 0 Loss 0.2252 Accuracy 0.3778\n",
            "Epoch 78 Batch 50 Loss 0.3114 Accuracy 0.3604\n",
            "Epoch 78 Batch 100 Loss 0.3518 Accuracy 0.3562\n",
            "Epoch 78 Loss 0.3650 Accuracy 0.3548\n",
            "Epoch 79 Batch 0 Loss 0.2810 Accuracy 0.3580\n",
            "Epoch 79 Batch 50 Loss 0.3197 Accuracy 0.3593\n",
            "Epoch 79 Batch 100 Loss 0.3556 Accuracy 0.3563\n",
            "Epoch 79 Loss 0.3694 Accuracy 0.3543\n",
            "Epoch 80 Batch 0 Loss 0.2216 Accuracy 0.3878\n",
            "Epoch 80 Batch 50 Loss 0.3161 Accuracy 0.3600\n",
            "Epoch 80 Batch 100 Loss 0.3492 Accuracy 0.3571\n",
            "Epoch 80 Loss 0.3646 Accuracy 0.3546\n",
            "Epoch 81 Batch 0 Loss 0.2778 Accuracy 0.3693\n",
            "Epoch 81 Batch 50 Loss 0.3080 Accuracy 0.3604\n",
            "Epoch 81 Batch 100 Loss 0.3500 Accuracy 0.3562\n",
            "Epoch 81 Loss 0.3637 Accuracy 0.3542\n",
            "Epoch 82 Batch 0 Loss 0.2696 Accuracy 0.3622\n",
            "Epoch 82 Batch 50 Loss 0.3143 Accuracy 0.3606\n",
            "Epoch 82 Batch 100 Loss 0.3484 Accuracy 0.3564\n",
            "Epoch 82 Loss 0.3633 Accuracy 0.3542\n",
            "Epoch 83 Batch 0 Loss 0.3343 Accuracy 0.3622\n",
            "Epoch 83 Batch 50 Loss 0.3087 Accuracy 0.3614\n",
            "Epoch 83 Batch 100 Loss 0.3445 Accuracy 0.3568\n",
            "Epoch 83 Loss 0.3598 Accuracy 0.3546\n",
            "Epoch 84 Batch 0 Loss 0.2353 Accuracy 0.3736\n",
            "Epoch 84 Batch 50 Loss 0.3071 Accuracy 0.3612\n",
            "Epoch 84 Batch 100 Loss 0.3442 Accuracy 0.3572\n",
            "Epoch 84 Loss 0.3594 Accuracy 0.3553\n",
            "Epoch 85 Batch 0 Loss 0.2730 Accuracy 0.3722\n",
            "Epoch 85 Batch 50 Loss 0.3069 Accuracy 0.3613\n",
            "Epoch 85 Batch 100 Loss 0.3392 Accuracy 0.3573\n",
            "Epoch 85 Loss 0.3542 Accuracy 0.3551\n",
            "Epoch 86 Batch 0 Loss 0.2247 Accuracy 0.3778\n",
            "Epoch 86 Batch 50 Loss 0.3079 Accuracy 0.3617\n",
            "Epoch 86 Batch 100 Loss 0.3404 Accuracy 0.3567\n",
            "Epoch 86 Loss 0.3542 Accuracy 0.3553\n",
            "Epoch 87 Batch 0 Loss 0.2866 Accuracy 0.3580\n",
            "Epoch 87 Batch 50 Loss 0.3060 Accuracy 0.3598\n",
            "Epoch 87 Batch 100 Loss 0.3395 Accuracy 0.3559\n",
            "Epoch 87 Loss 0.3532 Accuracy 0.3550\n",
            "Epoch 88 Batch 0 Loss 0.2437 Accuracy 0.3707\n",
            "Epoch 88 Batch 50 Loss 0.3054 Accuracy 0.3604\n",
            "Epoch 88 Batch 100 Loss 0.3382 Accuracy 0.3569\n",
            "Epoch 88 Loss 0.3533 Accuracy 0.3552\n",
            "Epoch 89 Batch 0 Loss 0.2273 Accuracy 0.3722\n",
            "Epoch 89 Batch 50 Loss 0.2992 Accuracy 0.3600\n",
            "Epoch 89 Batch 100 Loss 0.3345 Accuracy 0.3573\n",
            "Epoch 89 Loss 0.3501 Accuracy 0.3560\n",
            "Epoch 90 Batch 0 Loss 0.3163 Accuracy 0.3622\n",
            "Epoch 90 Batch 50 Loss 0.3021 Accuracy 0.3599\n",
            "Epoch 90 Batch 100 Loss 0.3312 Accuracy 0.3574\n",
            "Epoch 90 Loss 0.3481 Accuracy 0.3556\n",
            "Epoch 91 Batch 0 Loss 0.2656 Accuracy 0.3764\n",
            "Epoch 91 Batch 50 Loss 0.2963 Accuracy 0.3612\n",
            "Epoch 91 Batch 100 Loss 0.3308 Accuracy 0.3581\n",
            "Epoch 91 Loss 0.3463 Accuracy 0.3554\n",
            "Epoch 92 Batch 0 Loss 0.2463 Accuracy 0.3580\n",
            "Epoch 92 Batch 50 Loss 0.3012 Accuracy 0.3612\n",
            "Epoch 92 Batch 100 Loss 0.3308 Accuracy 0.3569\n",
            "Epoch 92 Loss 0.3419 Accuracy 0.3559\n",
            "Epoch 93 Batch 0 Loss 0.2565 Accuracy 0.3864\n",
            "Epoch 93 Batch 50 Loss 0.2981 Accuracy 0.3597\n",
            "Epoch 93 Batch 100 Loss 0.3318 Accuracy 0.3565\n",
            "Epoch 93 Loss 0.3448 Accuracy 0.3553\n",
            "Epoch 94 Batch 0 Loss 0.2355 Accuracy 0.3594\n",
            "Epoch 94 Batch 50 Loss 0.2949 Accuracy 0.3612\n",
            "Epoch 94 Batch 100 Loss 0.3303 Accuracy 0.3567\n",
            "Epoch 94 Loss 0.3438 Accuracy 0.3551\n",
            "Epoch 95 Batch 0 Loss 0.2790 Accuracy 0.3565\n",
            "Epoch 95 Batch 50 Loss 0.2987 Accuracy 0.3597\n",
            "Epoch 95 Batch 100 Loss 0.3290 Accuracy 0.3567\n",
            "Epoch 95 Loss 0.3425 Accuracy 0.3554\n",
            "Epoch 96 Batch 0 Loss 0.2985 Accuracy 0.3793\n",
            "Epoch 96 Batch 50 Loss 0.2942 Accuracy 0.3614\n",
            "Epoch 96 Batch 100 Loss 0.3308 Accuracy 0.3566\n",
            "Epoch 96 Loss 0.3417 Accuracy 0.3553\n",
            "Epoch 97 Batch 0 Loss 0.2341 Accuracy 0.3608\n",
            "Epoch 97 Batch 50 Loss 0.2974 Accuracy 0.3612\n",
            "Epoch 97 Batch 100 Loss 0.3272 Accuracy 0.3569\n",
            "Epoch 97 Loss 0.3416 Accuracy 0.3557\n",
            "Epoch 98 Batch 0 Loss 0.2372 Accuracy 0.3736\n",
            "Epoch 98 Batch 50 Loss 0.2834 Accuracy 0.3634\n",
            "Epoch 98 Batch 100 Loss 0.3208 Accuracy 0.3580\n",
            "Epoch 98 Loss 0.3377 Accuracy 0.3556\n",
            "Epoch 99 Batch 0 Loss 0.3220 Accuracy 0.3480\n",
            "Epoch 99 Batch 50 Loss 0.2950 Accuracy 0.3617\n",
            "Epoch 99 Batch 100 Loss 0.3278 Accuracy 0.3581\n",
            "Epoch 99 Loss 0.3384 Accuracy 0.3564\n",
            "Epoch 100 Batch 0 Loss 0.2371 Accuracy 0.3523\n",
            "Epoch 100 Batch 50 Loss 0.2898 Accuracy 0.3613\n",
            "Epoch 100 Batch 100 Loss 0.3273 Accuracy 0.3571\n",
            "Epoch 100 Loss 0.3380 Accuracy 0.3560\n",
            "Epoch 101 Batch 0 Loss 0.2429 Accuracy 0.3580\n",
            "Epoch 101 Batch 50 Loss 0.2901 Accuracy 0.3608\n",
            "Epoch 101 Batch 100 Loss 0.3259 Accuracy 0.3557\n",
            "Epoch 101 Loss 0.3367 Accuracy 0.3554\n",
            "Epoch 102 Batch 0 Loss 0.2528 Accuracy 0.3622\n",
            "Epoch 102 Batch 50 Loss 0.2895 Accuracy 0.3615\n",
            "Epoch 102 Batch 100 Loss 0.3204 Accuracy 0.3570\n",
            "Epoch 102 Loss 0.3320 Accuracy 0.3558\n",
            "Epoch 103 Batch 0 Loss 0.2454 Accuracy 0.3750\n",
            "Epoch 103 Batch 50 Loss 0.2923 Accuracy 0.3610\n",
            "Epoch 103 Batch 100 Loss 0.3210 Accuracy 0.3572\n",
            "Epoch 103 Loss 0.3348 Accuracy 0.3555\n",
            "Epoch 104 Batch 0 Loss 0.2256 Accuracy 0.3892\n",
            "Epoch 104 Batch 50 Loss 0.2969 Accuracy 0.3601\n",
            "Epoch 104 Batch 100 Loss 0.3238 Accuracy 0.3562\n",
            "Epoch 104 Loss 0.3334 Accuracy 0.3560\n",
            "Epoch 105 Batch 0 Loss 0.1795 Accuracy 0.3835\n",
            "Epoch 105 Batch 50 Loss 0.2808 Accuracy 0.3637\n",
            "Epoch 105 Batch 100 Loss 0.3166 Accuracy 0.3579\n",
            "Epoch 105 Loss 0.3310 Accuracy 0.3561\n",
            "Epoch 106 Batch 0 Loss 0.2646 Accuracy 0.3679\n",
            "Epoch 106 Batch 50 Loss 0.2881 Accuracy 0.3624\n",
            "Epoch 106 Batch 100 Loss 0.3205 Accuracy 0.3575\n",
            "Epoch 106 Loss 0.3341 Accuracy 0.3558\n",
            "Epoch 107 Batch 0 Loss 0.2483 Accuracy 0.3793\n",
            "Epoch 107 Batch 50 Loss 0.2894 Accuracy 0.3602\n",
            "Epoch 107 Batch 100 Loss 0.3193 Accuracy 0.3569\n",
            "Epoch 107 Loss 0.3280 Accuracy 0.3559\n",
            "Epoch 108 Batch 0 Loss 0.2379 Accuracy 0.3750\n",
            "Epoch 108 Batch 50 Loss 0.2722 Accuracy 0.3632\n",
            "Epoch 108 Batch 100 Loss 0.3137 Accuracy 0.3578\n",
            "Epoch 108 Loss 0.3281 Accuracy 0.3561\n",
            "Epoch 109 Batch 0 Loss 0.2612 Accuracy 0.3594\n",
            "Epoch 109 Batch 50 Loss 0.2815 Accuracy 0.3605\n",
            "Epoch 109 Batch 100 Loss 0.3141 Accuracy 0.3574\n",
            "Epoch 109 Loss 0.3240 Accuracy 0.3564\n",
            "Epoch 110 Batch 0 Loss 0.2271 Accuracy 0.3722\n",
            "Epoch 110 Batch 50 Loss 0.2848 Accuracy 0.3608\n",
            "Epoch 110 Batch 100 Loss 0.3114 Accuracy 0.3581\n",
            "Epoch 110 Loss 0.3243 Accuracy 0.3566\n",
            "Epoch 111 Batch 0 Loss 0.2155 Accuracy 0.3722\n",
            "Epoch 111 Batch 50 Loss 0.2810 Accuracy 0.3606\n",
            "Epoch 111 Batch 100 Loss 0.3121 Accuracy 0.3579\n",
            "Epoch 111 Loss 0.3267 Accuracy 0.3561\n",
            "Epoch 112 Batch 0 Loss 0.2127 Accuracy 0.3466\n",
            "Epoch 112 Batch 50 Loss 0.2753 Accuracy 0.3628\n",
            "Epoch 112 Batch 100 Loss 0.3083 Accuracy 0.3578\n",
            "Epoch 112 Loss 0.3218 Accuracy 0.3565\n",
            "Epoch 113 Batch 0 Loss 0.2531 Accuracy 0.3523\n",
            "Epoch 113 Batch 50 Loss 0.2773 Accuracy 0.3631\n",
            "Epoch 113 Batch 100 Loss 0.3109 Accuracy 0.3579\n",
            "Epoch 113 Loss 0.3240 Accuracy 0.3564\n",
            "Epoch 114 Batch 0 Loss 0.2803 Accuracy 0.3537\n",
            "Epoch 114 Batch 50 Loss 0.2854 Accuracy 0.3576\n",
            "Epoch 114 Batch 100 Loss 0.3114 Accuracy 0.3573\n",
            "Epoch 114 Loss 0.3220 Accuracy 0.3563\n",
            "Epoch 115 Batch 0 Loss 0.2304 Accuracy 0.3722\n",
            "Epoch 115 Batch 50 Loss 0.2787 Accuracy 0.3607\n",
            "Epoch 115 Batch 100 Loss 0.3100 Accuracy 0.3569\n",
            "Epoch 115 Loss 0.3218 Accuracy 0.3559\n",
            "Epoch 116 Batch 0 Loss 0.2526 Accuracy 0.3707\n",
            "Epoch 116 Batch 50 Loss 0.2780 Accuracy 0.3609\n",
            "Epoch 116 Batch 100 Loss 0.3096 Accuracy 0.3578\n",
            "Epoch 116 Loss 0.3183 Accuracy 0.3566\n",
            "Epoch 117 Batch 0 Loss 0.2173 Accuracy 0.3722\n",
            "Epoch 117 Batch 50 Loss 0.2706 Accuracy 0.3642\n",
            "Epoch 117 Batch 100 Loss 0.3099 Accuracy 0.3573\n",
            "Epoch 117 Loss 0.3200 Accuracy 0.3560\n",
            "Epoch 118 Batch 0 Loss 0.2699 Accuracy 0.3565\n",
            "Epoch 118 Batch 50 Loss 0.2718 Accuracy 0.3623\n",
            "Epoch 118 Batch 100 Loss 0.3057 Accuracy 0.3584\n",
            "Epoch 118 Loss 0.3182 Accuracy 0.3568\n",
            "Epoch 119 Batch 0 Loss 0.2374 Accuracy 0.3608\n",
            "Epoch 119 Batch 50 Loss 0.2718 Accuracy 0.3628\n",
            "Epoch 119 Batch 100 Loss 0.3004 Accuracy 0.3584\n",
            "Epoch 119 Loss 0.3151 Accuracy 0.3566\n",
            "Epoch 120 Batch 0 Loss 0.2678 Accuracy 0.3523\n",
            "Epoch 120 Batch 50 Loss 0.2647 Accuracy 0.3624\n",
            "Epoch 120 Batch 100 Loss 0.3034 Accuracy 0.3588\n",
            "Epoch 120 Loss 0.3143 Accuracy 0.3574\n",
            "Epoch 121 Batch 0 Loss 0.2187 Accuracy 0.3693\n",
            "Epoch 121 Batch 50 Loss 0.2766 Accuracy 0.3618\n",
            "Epoch 121 Batch 100 Loss 0.3071 Accuracy 0.3582\n",
            "Epoch 121 Loss 0.3186 Accuracy 0.3563\n",
            "Epoch 122 Batch 0 Loss 0.3103 Accuracy 0.3651\n",
            "Epoch 122 Batch 50 Loss 0.2683 Accuracy 0.3620\n",
            "Epoch 122 Batch 100 Loss 0.3012 Accuracy 0.3589\n",
            "Epoch 122 Loss 0.3143 Accuracy 0.3571\n",
            "Epoch 123 Batch 0 Loss 0.2170 Accuracy 0.3849\n",
            "Epoch 123 Batch 50 Loss 0.2727 Accuracy 0.3622\n",
            "Epoch 123 Batch 100 Loss 0.3057 Accuracy 0.3582\n",
            "Epoch 123 Loss 0.3165 Accuracy 0.3568\n",
            "Epoch 124 Batch 0 Loss 0.2204 Accuracy 0.3736\n",
            "Epoch 124 Batch 50 Loss 0.2711 Accuracy 0.3622\n",
            "Epoch 124 Batch 100 Loss 0.3031 Accuracy 0.3582\n",
            "Epoch 124 Loss 0.3174 Accuracy 0.3563\n",
            "Epoch 125 Batch 0 Loss 0.3084 Accuracy 0.3494\n",
            "Epoch 125 Batch 50 Loss 0.2724 Accuracy 0.3635\n",
            "Epoch 125 Batch 100 Loss 0.2979 Accuracy 0.3590\n",
            "Epoch 125 Loss 0.3106 Accuracy 0.3574\n",
            "Epoch 126 Batch 0 Loss 0.2202 Accuracy 0.3793\n",
            "Epoch 126 Batch 50 Loss 0.2690 Accuracy 0.3652\n",
            "Epoch 126 Batch 100 Loss 0.3002 Accuracy 0.3594\n",
            "Epoch 126 Loss 0.3141 Accuracy 0.3574\n",
            "Epoch 127 Batch 0 Loss 0.2461 Accuracy 0.3707\n",
            "Epoch 127 Batch 50 Loss 0.2685 Accuracy 0.3616\n",
            "Epoch 127 Batch 100 Loss 0.2976 Accuracy 0.3577\n",
            "Epoch 127 Loss 0.3117 Accuracy 0.3568\n",
            "Epoch 128 Batch 0 Loss 0.2302 Accuracy 0.3878\n",
            "Epoch 128 Batch 50 Loss 0.2760 Accuracy 0.3612\n",
            "Epoch 128 Batch 100 Loss 0.3029 Accuracy 0.3580\n",
            "Epoch 128 Loss 0.3142 Accuracy 0.3568\n",
            "Epoch 129 Batch 0 Loss 0.2611 Accuracy 0.3480\n",
            "Epoch 129 Batch 50 Loss 0.2650 Accuracy 0.3660\n",
            "Epoch 129 Batch 100 Loss 0.2978 Accuracy 0.3596\n",
            "Epoch 129 Loss 0.3098 Accuracy 0.3570\n",
            "Epoch 130 Batch 0 Loss 0.2796 Accuracy 0.3679\n",
            "Epoch 130 Batch 50 Loss 0.2702 Accuracy 0.3636\n",
            "Epoch 130 Batch 100 Loss 0.3020 Accuracy 0.3582\n",
            "Epoch 130 Loss 0.3137 Accuracy 0.3568\n",
            "Epoch 131 Batch 0 Loss 0.2305 Accuracy 0.3693\n",
            "Epoch 131 Batch 50 Loss 0.2719 Accuracy 0.3632\n",
            "Epoch 131 Batch 100 Loss 0.3014 Accuracy 0.3581\n",
            "Epoch 131 Loss 0.3113 Accuracy 0.3569\n",
            "Epoch 132 Batch 0 Loss 0.2204 Accuracy 0.3537\n",
            "Epoch 132 Batch 50 Loss 0.2672 Accuracy 0.3632\n",
            "Epoch 132 Batch 100 Loss 0.2963 Accuracy 0.3587\n",
            "Epoch 132 Loss 0.3088 Accuracy 0.3568\n",
            "Epoch 133 Batch 0 Loss 0.2787 Accuracy 0.3750\n",
            "Epoch 133 Batch 50 Loss 0.2682 Accuracy 0.3635\n",
            "Epoch 133 Batch 100 Loss 0.2950 Accuracy 0.3597\n",
            "Epoch 133 Loss 0.3095 Accuracy 0.3570\n",
            "Epoch 134 Batch 0 Loss 0.2413 Accuracy 0.3707\n",
            "Epoch 134 Batch 50 Loss 0.2710 Accuracy 0.3626\n",
            "Epoch 134 Batch 100 Loss 0.2982 Accuracy 0.3579\n",
            "Epoch 134 Loss 0.3087 Accuracy 0.3568\n",
            "Epoch 135 Batch 0 Loss 0.2131 Accuracy 0.3651\n",
            "Epoch 135 Batch 50 Loss 0.2644 Accuracy 0.3630\n",
            "Epoch 135 Batch 100 Loss 0.2970 Accuracy 0.3573\n",
            "Epoch 135 Loss 0.3075 Accuracy 0.3566\n",
            "Epoch 136 Batch 0 Loss 0.2094 Accuracy 0.3608\n",
            "Epoch 136 Batch 50 Loss 0.2595 Accuracy 0.3622\n",
            "Epoch 136 Batch 100 Loss 0.2976 Accuracy 0.3576\n",
            "Epoch 136 Loss 0.3061 Accuracy 0.3569\n",
            "Epoch 137 Batch 0 Loss 0.2285 Accuracy 0.3651\n",
            "Epoch 137 Batch 50 Loss 0.2650 Accuracy 0.3626\n",
            "Epoch 137 Batch 100 Loss 0.2905 Accuracy 0.3590\n",
            "Epoch 137 Loss 0.3051 Accuracy 0.3573\n",
            "Epoch 138 Batch 0 Loss 0.2785 Accuracy 0.3509\n",
            "Epoch 138 Batch 50 Loss 0.2663 Accuracy 0.3636\n",
            "Epoch 138 Batch 100 Loss 0.2933 Accuracy 0.3603\n",
            "Epoch 138 Loss 0.3062 Accuracy 0.3578\n",
            "Epoch 139 Batch 0 Loss 0.2250 Accuracy 0.3608\n",
            "Epoch 139 Batch 50 Loss 0.2607 Accuracy 0.3614\n",
            "Epoch 139 Batch 100 Loss 0.2912 Accuracy 0.3583\n",
            "Epoch 139 Loss 0.3039 Accuracy 0.3566\n",
            "Epoch 140 Batch 0 Loss 0.2198 Accuracy 0.3736\n",
            "Epoch 140 Batch 50 Loss 0.2723 Accuracy 0.3595\n",
            "Epoch 140 Batch 100 Loss 0.2916 Accuracy 0.3581\n",
            "Epoch 140 Loss 0.3028 Accuracy 0.3566\n",
            "Epoch 141 Batch 0 Loss 0.2224 Accuracy 0.3693\n",
            "Epoch 141 Batch 50 Loss 0.2663 Accuracy 0.3611\n",
            "Epoch 141 Batch 100 Loss 0.2915 Accuracy 0.3586\n",
            "Epoch 141 Loss 0.3049 Accuracy 0.3568\n",
            "Epoch 142 Batch 0 Loss 0.2606 Accuracy 0.3565\n",
            "Epoch 142 Batch 50 Loss 0.2651 Accuracy 0.3626\n",
            "Epoch 142 Batch 100 Loss 0.2905 Accuracy 0.3580\n",
            "Epoch 142 Loss 0.2995 Accuracy 0.3575\n",
            "Epoch 143 Batch 0 Loss 0.2863 Accuracy 0.3509\n",
            "Epoch 143 Batch 50 Loss 0.2626 Accuracy 0.3630\n",
            "Epoch 143 Batch 100 Loss 0.2914 Accuracy 0.3580\n",
            "Epoch 143 Loss 0.3027 Accuracy 0.3566\n",
            "Epoch 144 Batch 0 Loss 0.2467 Accuracy 0.3693\n",
            "Epoch 144 Batch 50 Loss 0.2646 Accuracy 0.3624\n",
            "Epoch 144 Batch 100 Loss 0.2932 Accuracy 0.3582\n",
            "Epoch 144 Loss 0.3043 Accuracy 0.3572\n",
            "Epoch 145 Batch 0 Loss 0.1955 Accuracy 0.3864\n",
            "Epoch 145 Batch 50 Loss 0.2622 Accuracy 0.3623\n",
            "Epoch 145 Batch 100 Loss 0.2887 Accuracy 0.3589\n",
            "Epoch 145 Loss 0.3007 Accuracy 0.3571\n",
            "Epoch 146 Batch 0 Loss 0.2315 Accuracy 0.3679\n",
            "Epoch 146 Batch 50 Loss 0.2604 Accuracy 0.3634\n",
            "Epoch 146 Batch 100 Loss 0.2906 Accuracy 0.3579\n",
            "Epoch 146 Loss 0.2991 Accuracy 0.3576\n",
            "Epoch 147 Batch 0 Loss 0.2998 Accuracy 0.3565\n",
            "Epoch 147 Batch 50 Loss 0.2577 Accuracy 0.3634\n",
            "Epoch 147 Batch 100 Loss 0.2882 Accuracy 0.3593\n",
            "Epoch 147 Loss 0.2985 Accuracy 0.3573\n",
            "Epoch 148 Batch 0 Loss 0.2438 Accuracy 0.3679\n",
            "Epoch 148 Batch 50 Loss 0.2517 Accuracy 0.3648\n",
            "Epoch 148 Batch 100 Loss 0.2854 Accuracy 0.3599\n",
            "Epoch 148 Loss 0.2972 Accuracy 0.3581\n",
            "Epoch 149 Batch 0 Loss 0.2294 Accuracy 0.3494\n",
            "Epoch 149 Batch 50 Loss 0.2542 Accuracy 0.3607\n",
            "Epoch 149 Batch 100 Loss 0.2849 Accuracy 0.3582\n",
            "Epoch 149 Loss 0.2990 Accuracy 0.3568\n",
            "Epoch 150 Batch 0 Loss 0.2586 Accuracy 0.3651\n",
            "Epoch 150 Batch 50 Loss 0.2581 Accuracy 0.3631\n",
            "Epoch 150 Batch 100 Loss 0.2879 Accuracy 0.3589\n",
            "Epoch 150 Loss 0.2981 Accuracy 0.3575\n",
            "Epoch 151 Batch 0 Loss 0.2844 Accuracy 0.3466\n",
            "Epoch 151 Batch 50 Loss 0.2589 Accuracy 0.3632\n",
            "Epoch 151 Batch 100 Loss 0.2855 Accuracy 0.3589\n",
            "Epoch 151 Loss 0.2967 Accuracy 0.3571\n",
            "Epoch 152 Batch 0 Loss 0.2228 Accuracy 0.3849\n",
            "Epoch 152 Batch 50 Loss 0.2560 Accuracy 0.3627\n",
            "Epoch 152 Batch 100 Loss 0.2842 Accuracy 0.3602\n",
            "Epoch 152 Loss 0.2999 Accuracy 0.3574\n",
            "Epoch 153 Batch 0 Loss 0.2359 Accuracy 0.3665\n",
            "Epoch 153 Batch 50 Loss 0.2571 Accuracy 0.3633\n",
            "Epoch 153 Batch 100 Loss 0.2843 Accuracy 0.3593\n",
            "Epoch 153 Loss 0.2963 Accuracy 0.3570\n",
            "Epoch 154 Batch 0 Loss 0.2370 Accuracy 0.3636\n",
            "Epoch 154 Batch 50 Loss 0.2603 Accuracy 0.3611\n",
            "Epoch 154 Batch 100 Loss 0.2822 Accuracy 0.3590\n",
            "Epoch 154 Loss 0.2952 Accuracy 0.3572\n",
            "Epoch 155 Batch 0 Loss 0.2574 Accuracy 0.3551\n",
            "Epoch 155 Batch 50 Loss 0.2557 Accuracy 0.3646\n",
            "Epoch 155 Batch 100 Loss 0.2808 Accuracy 0.3594\n",
            "Epoch 155 Loss 0.2932 Accuracy 0.3578\n",
            "Epoch 156 Batch 0 Loss 0.2084 Accuracy 0.3509\n",
            "Epoch 156 Batch 50 Loss 0.2581 Accuracy 0.3617\n",
            "Epoch 156 Batch 100 Loss 0.2813 Accuracy 0.3604\n",
            "Epoch 156 Loss 0.2947 Accuracy 0.3577\n",
            "Epoch 157 Batch 0 Loss 0.2076 Accuracy 0.3693\n",
            "Epoch 157 Batch 50 Loss 0.2569 Accuracy 0.3610\n",
            "Epoch 157 Batch 100 Loss 0.2802 Accuracy 0.3592\n",
            "Epoch 157 Loss 0.2950 Accuracy 0.3573\n",
            "Epoch 158 Batch 0 Loss 0.2874 Accuracy 0.3409\n",
            "Epoch 158 Batch 50 Loss 0.2603 Accuracy 0.3636\n",
            "Epoch 158 Batch 100 Loss 0.2842 Accuracy 0.3591\n",
            "Epoch 158 Loss 0.2958 Accuracy 0.3574\n",
            "Epoch 159 Batch 0 Loss 0.2896 Accuracy 0.3409\n",
            "Epoch 159 Batch 50 Loss 0.2536 Accuracy 0.3635\n",
            "Epoch 159 Batch 100 Loss 0.2857 Accuracy 0.3592\n",
            "Epoch 159 Loss 0.2971 Accuracy 0.3569\n",
            "Epoch 160 Batch 0 Loss 0.2272 Accuracy 0.3750\n",
            "Epoch 160 Batch 50 Loss 0.2547 Accuracy 0.3613\n",
            "Epoch 160 Batch 100 Loss 0.2800 Accuracy 0.3588\n",
            "Epoch 160 Loss 0.2909 Accuracy 0.3574\n",
            "Epoch 161 Batch 0 Loss 0.2417 Accuracy 0.3764\n",
            "Epoch 161 Batch 50 Loss 0.2547 Accuracy 0.3634\n",
            "Epoch 161 Batch 100 Loss 0.2807 Accuracy 0.3589\n",
            "Epoch 161 Loss 0.2918 Accuracy 0.3568\n",
            "Epoch 162 Batch 0 Loss 0.2190 Accuracy 0.3750\n",
            "Epoch 162 Batch 50 Loss 0.2527 Accuracy 0.3613\n",
            "Epoch 162 Batch 100 Loss 0.2798 Accuracy 0.3588\n",
            "Epoch 162 Loss 0.2893 Accuracy 0.3577\n",
            "Epoch 163 Batch 0 Loss 0.2197 Accuracy 0.3693\n",
            "Epoch 163 Batch 50 Loss 0.2561 Accuracy 0.3649\n",
            "Epoch 163 Batch 100 Loss 0.2781 Accuracy 0.3593\n",
            "Epoch 163 Loss 0.2921 Accuracy 0.3571\n",
            "Epoch 164 Batch 0 Loss 0.2270 Accuracy 0.3523\n",
            "Epoch 164 Batch 50 Loss 0.2547 Accuracy 0.3622\n",
            "Epoch 164 Batch 100 Loss 0.2813 Accuracy 0.3580\n",
            "Epoch 164 Loss 0.2912 Accuracy 0.3567\n",
            "Epoch 165 Batch 0 Loss 0.2111 Accuracy 0.3750\n",
            "Epoch 165 Batch 50 Loss 0.2509 Accuracy 0.3630\n",
            "Epoch 165 Batch 100 Loss 0.2766 Accuracy 0.3593\n",
            "Epoch 165 Loss 0.2892 Accuracy 0.3574\n",
            "Epoch 166 Batch 0 Loss 0.2483 Accuracy 0.3807\n",
            "Epoch 166 Batch 50 Loss 0.2489 Accuracy 0.3656\n",
            "Epoch 166 Batch 100 Loss 0.2785 Accuracy 0.3604\n",
            "Epoch 166 Loss 0.2916 Accuracy 0.3580\n",
            "Epoch 167 Batch 0 Loss 0.2642 Accuracy 0.3722\n",
            "Epoch 167 Batch 50 Loss 0.2565 Accuracy 0.3631\n",
            "Epoch 167 Batch 100 Loss 0.2808 Accuracy 0.3594\n",
            "Epoch 167 Loss 0.2912 Accuracy 0.3577\n",
            "Epoch 168 Batch 0 Loss 0.2687 Accuracy 0.3565\n",
            "Epoch 168 Batch 50 Loss 0.2526 Accuracy 0.3651\n",
            "Epoch 168 Batch 100 Loss 0.2767 Accuracy 0.3605\n",
            "Epoch 168 Loss 0.2894 Accuracy 0.3577\n",
            "Epoch 169 Batch 0 Loss 0.2308 Accuracy 0.3778\n",
            "Epoch 169 Batch 50 Loss 0.2487 Accuracy 0.3644\n",
            "Epoch 169 Batch 100 Loss 0.2786 Accuracy 0.3582\n",
            "Epoch 169 Loss 0.2883 Accuracy 0.3579\n",
            "Epoch 170 Batch 0 Loss 0.2086 Accuracy 0.3665\n",
            "Epoch 170 Batch 50 Loss 0.2450 Accuracy 0.3630\n",
            "Epoch 170 Batch 100 Loss 0.2775 Accuracy 0.3588\n",
            "Epoch 170 Loss 0.2908 Accuracy 0.3573\n",
            "Epoch 171 Batch 0 Loss 0.2155 Accuracy 0.3793\n",
            "Epoch 171 Batch 50 Loss 0.2489 Accuracy 0.3639\n",
            "Epoch 171 Batch 100 Loss 0.2749 Accuracy 0.3594\n",
            "Epoch 171 Loss 0.2871 Accuracy 0.3576\n",
            "Epoch 172 Batch 0 Loss 0.2220 Accuracy 0.3679\n",
            "Epoch 172 Batch 50 Loss 0.2502 Accuracy 0.3624\n",
            "Epoch 172 Batch 100 Loss 0.2762 Accuracy 0.3594\n",
            "Epoch 172 Loss 0.2862 Accuracy 0.3577\n",
            "Epoch 173 Batch 0 Loss 0.2592 Accuracy 0.3594\n",
            "Epoch 173 Batch 50 Loss 0.2504 Accuracy 0.3630\n",
            "Epoch 173 Batch 100 Loss 0.2734 Accuracy 0.3583\n",
            "Epoch 173 Loss 0.2868 Accuracy 0.3572\n",
            "Epoch 174 Batch 0 Loss 0.2230 Accuracy 0.3594\n",
            "Epoch 174 Batch 50 Loss 0.2518 Accuracy 0.3622\n",
            "Epoch 174 Batch 100 Loss 0.2772 Accuracy 0.3589\n",
            "Epoch 174 Loss 0.2887 Accuracy 0.3575\n",
            "Epoch 175 Batch 0 Loss 0.2118 Accuracy 0.3565\n",
            "Epoch 175 Batch 50 Loss 0.2454 Accuracy 0.3620\n",
            "Epoch 175 Batch 100 Loss 0.2758 Accuracy 0.3579\n",
            "Epoch 175 Loss 0.2849 Accuracy 0.3573\n",
            "Epoch 176 Batch 0 Loss 0.1949 Accuracy 0.3665\n",
            "Epoch 176 Batch 50 Loss 0.2502 Accuracy 0.3632\n",
            "Epoch 176 Batch 100 Loss 0.2754 Accuracy 0.3591\n",
            "Epoch 176 Loss 0.2873 Accuracy 0.3576\n",
            "Epoch 177 Batch 0 Loss 0.1681 Accuracy 0.3636\n",
            "Epoch 177 Batch 50 Loss 0.2472 Accuracy 0.3632\n",
            "Epoch 177 Batch 100 Loss 0.2753 Accuracy 0.3593\n",
            "Epoch 177 Loss 0.2859 Accuracy 0.3575\n",
            "Epoch 178 Batch 0 Loss 0.2055 Accuracy 0.3807\n",
            "Epoch 178 Batch 50 Loss 0.2445 Accuracy 0.3653\n",
            "Epoch 178 Batch 100 Loss 0.2719 Accuracy 0.3601\n",
            "Epoch 178 Loss 0.2835 Accuracy 0.3583\n",
            "Epoch 179 Batch 0 Loss 0.2315 Accuracy 0.3707\n",
            "Epoch 179 Batch 50 Loss 0.2428 Accuracy 0.3662\n",
            "Epoch 179 Batch 100 Loss 0.2715 Accuracy 0.3600\n",
            "Epoch 179 Loss 0.2846 Accuracy 0.3579\n",
            "Epoch 180 Batch 0 Loss 0.1787 Accuracy 0.3565\n",
            "Epoch 180 Batch 50 Loss 0.2501 Accuracy 0.3628\n",
            "Epoch 180 Batch 100 Loss 0.2747 Accuracy 0.3586\n",
            "Epoch 180 Loss 0.2846 Accuracy 0.3575\n",
            "Epoch 181 Batch 0 Loss 0.1907 Accuracy 0.3935\n",
            "Epoch 181 Batch 50 Loss 0.2465 Accuracy 0.3626\n",
            "Epoch 181 Batch 100 Loss 0.2753 Accuracy 0.3587\n",
            "Epoch 181 Loss 0.2851 Accuracy 0.3574\n",
            "Epoch 182 Batch 0 Loss 0.2017 Accuracy 0.3736\n",
            "Epoch 182 Batch 50 Loss 0.2477 Accuracy 0.3607\n",
            "Epoch 182 Batch 100 Loss 0.2745 Accuracy 0.3578\n",
            "Epoch 182 Loss 0.2830 Accuracy 0.3573\n",
            "Epoch 183 Batch 0 Loss 0.2689 Accuracy 0.3438\n",
            "Epoch 183 Batch 50 Loss 0.2470 Accuracy 0.3638\n",
            "Epoch 183 Batch 100 Loss 0.2720 Accuracy 0.3597\n",
            "Epoch 183 Loss 0.2848 Accuracy 0.3575\n",
            "Epoch 184 Batch 0 Loss 0.2733 Accuracy 0.3651\n",
            "Epoch 184 Batch 50 Loss 0.2476 Accuracy 0.3639\n",
            "Epoch 184 Batch 100 Loss 0.2727 Accuracy 0.3592\n",
            "Epoch 184 Loss 0.2839 Accuracy 0.3575\n",
            "Epoch 185 Batch 0 Loss 0.2233 Accuracy 0.3651\n",
            "Epoch 185 Batch 50 Loss 0.2434 Accuracy 0.3633\n",
            "Epoch 185 Batch 100 Loss 0.2720 Accuracy 0.3590\n",
            "Epoch 185 Loss 0.2812 Accuracy 0.3577\n",
            "Epoch 186 Batch 0 Loss 0.2110 Accuracy 0.3608\n",
            "Epoch 186 Batch 50 Loss 0.2518 Accuracy 0.3617\n",
            "Epoch 186 Batch 100 Loss 0.2706 Accuracy 0.3595\n",
            "Epoch 186 Loss 0.2822 Accuracy 0.3577\n",
            "Epoch 187 Batch 0 Loss 0.2251 Accuracy 0.3665\n",
            "Epoch 187 Batch 50 Loss 0.2431 Accuracy 0.3651\n",
            "Epoch 187 Batch 100 Loss 0.2703 Accuracy 0.3591\n",
            "Epoch 187 Loss 0.2823 Accuracy 0.3579\n",
            "Epoch 188 Batch 0 Loss 0.1958 Accuracy 0.3778\n",
            "Epoch 188 Batch 50 Loss 0.2485 Accuracy 0.3619\n",
            "Epoch 188 Batch 100 Loss 0.2702 Accuracy 0.3581\n",
            "Epoch 188 Loss 0.2806 Accuracy 0.3575\n",
            "Epoch 189 Batch 0 Loss 0.2270 Accuracy 0.3580\n",
            "Epoch 189 Batch 50 Loss 0.2450 Accuracy 0.3635\n",
            "Epoch 189 Batch 100 Loss 0.2709 Accuracy 0.3589\n",
            "Epoch 189 Loss 0.2823 Accuracy 0.3575\n",
            "Epoch 190 Batch 0 Loss 0.2542 Accuracy 0.3494\n",
            "Epoch 190 Batch 50 Loss 0.2395 Accuracy 0.3656\n",
            "Epoch 190 Batch 100 Loss 0.2663 Accuracy 0.3596\n",
            "Epoch 190 Loss 0.2788 Accuracy 0.3583\n",
            "Epoch 191 Batch 0 Loss 0.2533 Accuracy 0.3636\n",
            "Epoch 191 Batch 50 Loss 0.2425 Accuracy 0.3628\n",
            "Epoch 191 Batch 100 Loss 0.2693 Accuracy 0.3592\n",
            "Epoch 191 Loss 0.2809 Accuracy 0.3578\n",
            "Epoch 192 Batch 0 Loss 0.1912 Accuracy 0.3665\n",
            "Epoch 192 Batch 50 Loss 0.2388 Accuracy 0.3652\n",
            "Epoch 192 Batch 100 Loss 0.2668 Accuracy 0.3596\n",
            "Epoch 192 Loss 0.2787 Accuracy 0.3581\n",
            "Epoch 193 Batch 0 Loss 0.1855 Accuracy 0.3778\n",
            "Epoch 193 Batch 50 Loss 0.2412 Accuracy 0.3639\n",
            "Epoch 193 Batch 100 Loss 0.2695 Accuracy 0.3593\n",
            "Epoch 193 Loss 0.2804 Accuracy 0.3575\n",
            "Epoch 194 Batch 0 Loss 0.2485 Accuracy 0.3665\n",
            "Epoch 194 Batch 50 Loss 0.2427 Accuracy 0.3639\n",
            "Epoch 194 Batch 100 Loss 0.2698 Accuracy 0.3592\n",
            "Epoch 194 Loss 0.2790 Accuracy 0.3577\n",
            "Epoch 195 Batch 0 Loss 0.1929 Accuracy 0.3722\n",
            "Epoch 195 Batch 50 Loss 0.2443 Accuracy 0.3634\n",
            "Epoch 195 Batch 100 Loss 0.2667 Accuracy 0.3598\n",
            "Epoch 195 Loss 0.2780 Accuracy 0.3581\n",
            "Epoch 196 Batch 0 Loss 0.1917 Accuracy 0.3636\n",
            "Epoch 196 Batch 50 Loss 0.2410 Accuracy 0.3644\n",
            "Epoch 196 Batch 100 Loss 0.2676 Accuracy 0.3596\n",
            "Epoch 196 Loss 0.2780 Accuracy 0.3577\n",
            "Epoch 197 Batch 0 Loss 0.1692 Accuracy 0.3835\n",
            "Epoch 197 Batch 50 Loss 0.2406 Accuracy 0.3641\n",
            "Epoch 197 Batch 100 Loss 0.2696 Accuracy 0.3589\n",
            "Epoch 197 Loss 0.2772 Accuracy 0.3577\n",
            "Epoch 198 Batch 0 Loss 0.2245 Accuracy 0.3636\n",
            "Epoch 198 Batch 50 Loss 0.2355 Accuracy 0.3652\n",
            "Epoch 198 Batch 100 Loss 0.2673 Accuracy 0.3597\n",
            "Epoch 198 Loss 0.2783 Accuracy 0.3577\n",
            "Epoch 199 Batch 0 Loss 0.2664 Accuracy 0.3537\n",
            "Epoch 199 Batch 50 Loss 0.2374 Accuracy 0.3639\n",
            "Epoch 199 Batch 100 Loss 0.2703 Accuracy 0.3589\n",
            "Epoch 199 Loss 0.2775 Accuracy 0.3576\n",
            "Epoch 200 Batch 0 Loss 0.2161 Accuracy 0.3707\n",
            "Epoch 200 Batch 50 Loss 0.2417 Accuracy 0.3638\n",
            "Epoch 200 Batch 100 Loss 0.2696 Accuracy 0.3591\n",
            "Epoch 200 Loss 0.2776 Accuracy 0.3578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWlgQ0xyN4xH",
        "colab_type": "text"
      },
      "source": [
        "Очень похоже, что даже несмотря на увеличение количества эпох, accuracy не растёт. Впрочем, Loss падает и это уже хорошо."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KaTd_hxD4p-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    start_token = [1]\n",
        "    end_token = [2]\n",
        "  \n",
        "    sentence = preprocess_sentence(inp_sentence)\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    \n",
        "    encoder_input = tf.expand_dims(inputs, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "    decoder_input = [1]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_length_targ):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "\n",
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "    sentence = inp_lang_tokenizer.encode(sentence)\n",
        "  \n",
        "    attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "    for head in range(attention.shape[0]):\n",
        "        ax = fig.add_subplot(2, 4, head+1)\n",
        "\n",
        "        # plot the attention weights\n",
        "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "        fontdict = {'fontsize': 10}\n",
        "\n",
        "        ax.set_xticks(range(len(sentence)+2))\n",
        "        ax.set_yticks(range(len(result)))\n",
        "\n",
        "        ax.set_ylim(len(result)-1.5, -0.5)\n",
        "\n",
        "        ax.set_xticklabels(\n",
        "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
        "            fontdict=fontdict, rotation=90)\n",
        "\n",
        "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
        "                            if i < tokenizer_en.vocab_size], \n",
        "                           fontdict=fontdict)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def translate(sentence, plot=''):\n",
        "    result, attention_weights = evaluate(sentence)\n",
        "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  \n",
        "    if plot:\n",
        "        plot_attention_weights(attention_weights, sentence, result, plot)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p6DVj_JD85H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1c9f3e7-bf5f-4270-96bc-257148c7cb73"
      },
      "source": [
        "translate(\"good morning.\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: good morning.\n",
            "Predicted translation: ['<start>', 'сейчас', 'весна', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz_x3noDIm2-",
        "colab_type": "text"
      },
      "source": [
        "Мимо"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJompyz3IeW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e63a122-17f6-4fef-ffb0-c89b1ada584c"
      },
      "source": [
        "translate(\"lie down\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: lie down\n",
            "Predicted translation: ['<start>', 'рыбу', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoFLDREOIrR2",
        "colab_type": "text"
      },
      "source": [
        "Совсем, нет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXub3eq0InXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1386141b-29b6-4bba-9ffa-86b823c6f8f5"
      },
      "source": [
        "translate(\"have a nice day\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: have a nice day\n",
            "Predicted translation: ['<start>', 'долг', 'зовёт', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PQdQEG_R_73",
        "colab_type": "text"
      },
      "source": [
        "Опять мимо"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po5L494-IkzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13d1bfc9-efa9-418d-f8ee-8ed3abb6eb89"
      },
      "source": [
        "translate(\"hello\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: hello\n",
            "Predicted translation: ['<start>', 'здравствуйте', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUn7DcO2SCcU",
        "colab_type": "text"
      },
      "source": [
        "Ну, хоть что-то"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8x8K-bDSEgn",
        "colab_type": "text"
      },
      "source": [
        "На мой взгляд Attantion работает вполне годно. Возможно, транформер может больше, но что-то нужно подкрутить в настройках и обучается он тоже ощутимо дольше"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZputDXFzR6kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}